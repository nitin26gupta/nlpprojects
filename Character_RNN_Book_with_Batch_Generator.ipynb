{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. CHAR_RNN_BOOK with Batch Generator.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FjMJKaBXi9ru"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NdRMc0S0qZlv",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.reset_default_graph()\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QVb7_6pXsOZT"
      },
      "source": [
        "### Collect Data\n",
        "<font size=\"2\">Download data from Project Gutenberg site -> http://www.gutenberg.org/files/1342/1342-0.txt </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddmptSTEJE76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.gutenberg.org/files/1342/1342-0.txt --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "USE1lx0aqZl3",
        "colab": {}
      },
      "source": [
        "book_text = open('1342-0.txt', encoding='utf8').read()\n",
        "print('Length of the book: ' , len(book_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZcO2q06yVDr",
        "colab": {}
      },
      "source": [
        "book_text[1000:1100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jT0ptJlHqZmA"
      },
      "source": [
        "### Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WqoU8rqQqZmB",
        "colab": {}
      },
      "source": [
        "#Tokenize at character level\n",
        "t = tf.keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
        "\n",
        "#Fit tokenizer on the book\n",
        "t.fit_on_texts(book_text)\n",
        "\n",
        "#Vocablury size\n",
        "vocab_size = len(t.word_index)\n",
        "print('Number of unique characters: ', vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QakOBXfvsOZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert characters in the book to Numbers\n",
        "book_num = t.texts_to_sequences(book_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImHfcPQWsOZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build a dictionary which can convert numbers into chars\n",
        "int_to_char = dict((i,c) for c, i in t.word_index.items()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fhTY6QPdqZmY"
      },
      "source": [
        "### Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEYCq4G2x8Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoKJkk2FsOZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "record_num = 0 #starting batch number\n",
        "sequence_length = 100 #Length of input sequence\n",
        "\n",
        "def batch_generator(batch_size=32):\n",
        "    \n",
        "    #Will update batch_num\n",
        "    global record_num\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        #Empty list for input and output data\n",
        "        input_data = []\n",
        "        output_data = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            #input sequence\n",
        "            input_seq = book_num[(record_num * sequence_length) : (record_num * sequence_length) + sequence_length]\n",
        "            #Output sequence\n",
        "            output_seq = book_num[(record_num * sequence_length) + sequence_length]\n",
        "\n",
        "            input_data.append(input_seq)\n",
        "            output_data.append(output_seq)\n",
        "            \n",
        "            record_num = record_num + 1\n",
        "            \n",
        "            if((record_num*sequence_length + sequence_length) > len(book_num)):\n",
        "                record_num = 0\n",
        "                \n",
        "\n",
        "        #Input data one hot encoding\n",
        "        input_data = tf.keras.utils.to_categorical(input_data,num_classes=vocab_size+1)\n",
        "\n",
        "        #Output data one hot encoding\n",
        "        output_data = tf.keras.utils.to_categorical(output_data,num_classes=vocab_size+1)\n",
        " \n",
        "        print(\"len(input_data) : \",len(input_data))\n",
        "        print(\"sequence_length: \",sequence_length)\n",
        "        print(\"vocab_size: \",vocab_size)\n",
        "        print(\"input_data.shape: \",input_data.shape)\n",
        "        #Reshape input data into 3 dimensional numpy array\n",
        "        #batch_size x sequence_length x vocab_size+1\n",
        "        input_data = np.reshape(input_data,\n",
        "                                (len(input_data),\n",
        "                                 sequence_length,\n",
        "                                 vocab_size+1))\n",
        "        print(\"input_data.shape: \",input_data.shape)\n",
        "        yield input_data, output_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQdLOSVKKK3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = batch_generator()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FIrtT6dKUAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = next(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VENlON4KKQiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueB_oiHVKSPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "831Cc5dkqZm7"
      },
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXjF3ye4KNLS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FmHZZrnqZm8",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#LSTM\n",
        "model.add(tf.keras.layers.LSTM(256, input_shape=(sequence_length,vocab_size+1)))\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy') #No accuracy tracking here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlmHfhSUqZnW"
      },
      "source": [
        "### Print model output during Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAApCmYsOZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lyh80KAdpEXB",
        "colab": {}
      },
      "source": [
        "#Identify a random sequence which we will use to generate output\n",
        "start_pos = np.random.randint(0, high=(len(book_num) - sequence_length))\n",
        "test_seq =  book_num[start_pos : start_pos+sequence_length]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG1oMSXZAiUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fObNxWaeLdg5",
        "colab": {}
      },
      "source": [
        "#Print random starting sequence for prediction\n",
        "print('Initial sequence is: ')\n",
        "for i in range (sequence_length):\n",
        "    print(int_to_char[test_seq[i][0]], end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pMEA1TU4sOZu"
      },
      "source": [
        "### Prediction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ste0lSBs-igA",
        "colab": {}
      },
      "source": [
        "def predict_seq(epoch, logs):\n",
        "    \n",
        "    print('\\n\\nOutput sequence after epoch ', epoch, ' :')\n",
        "    \n",
        "    #Initialize predicted output\n",
        "    predicted_output = ''\n",
        "    \n",
        "    #lets predict 50 next chars\n",
        "    current_seq = np.copy(test_seq)\n",
        "    \n",
        "    for i in range(50):\n",
        "        current_seq_one_hot = tf.keras.utils.to_categorical(current_seq, num_classes=vocab_size+1)\n",
        "        \n",
        "        data_input = np.reshape(current_seq_one_hot,(1,\n",
        "                                                     current_seq_one_hot.shape[0],\n",
        "                                                     current_seq_one_hot.shape[1]))\n",
        "        \n",
        "        #Get the char int with maximum probability\n",
        "        predicted_char_int = np.argmax(model.predict(data_input)[0])\n",
        "        \n",
        "        if (predicted_char_int != 0):\n",
        "            \n",
        "            #Add to the predicted out, convert int to char\n",
        "            predicted_output = predicted_output + int_to_char[predicted_char_int]\n",
        "        \n",
        "        #Update seq with new value at the end\n",
        "        current_seq = np.roll(current_seq, -1)\n",
        "        current_seq[current_seq.shape[0]-1] = [predicted_char_int]\n",
        "    \n",
        "    print(predicted_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gwvPYhTpsOZw"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VW72ruTt1I9r",
        "colab": {}
      },
      "source": [
        "#Create a LabdaCallback to do prediction at end of every epoch\n",
        "lambda_checkpoint = tf.keras.callbacks.LambdaCallback(on_epoch_end=predict_seq)\n",
        "\n",
        "#Create a model checkpoint to store model after each epoch if loss reduces\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('char_rnn.h5',\n",
        "                                                      monitor='loss',\n",
        "                                                      save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HAVH-aqyqZna",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "batch_size = 2000\n",
        "train_generator = batch_generator(batch_size=batch_size)\n",
        "\n",
        "#Fit generator\n",
        "model.fit_generator(train_generator,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch = (len(book_num)- sequence_length)// batch_size,                    \n",
        "                    callbacks=[model_checkpoint, lambda_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMysYdbqK6lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}